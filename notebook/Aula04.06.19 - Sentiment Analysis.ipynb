{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis\n",
    "Read https://web.stanford.edu/~jurafsky/slp3/19.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlputils.lexical import Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "stopwords = stopwords.words('portuguese')\n",
    "normalizer = Preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download LIWC resource at http://143.107.183.175:21380/portlex/images/arquivos/liwc/LIWC2007_Portugues_win.dic.txt\n",
    "# posemo = 126\n",
    "# negemo = 127\n",
    "# what more?\n",
    "positives = []\n",
    "negatives = []\n",
    "\n",
    "with open('../models/LIWC2007_Portugues_win.dic.txt', 'r', encoding='latin') as liwc_file:\n",
    "    in_header = True\n",
    "    for line in liwc_file.readlines():\n",
    "        if not re.match('^\\d+', line):\n",
    "            parts = line.split()\n",
    "            word = parts.pop(0)\n",
    "            #rotulos de palavras positivoas\n",
    "            if '126' in parts:\n",
    "                positives.append(word)\n",
    "            #rotulos de palavras negativas\n",
    "            elif '127' in parts:\n",
    "                negatives.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizar outros rotulos alem dos anteriores que possam incorporar palavras positivas e negativas\n",
    "#tratar palavras com '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'otimamente' in positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'triste' in negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_sentment_analysis(text, binary=False):\n",
    "    text = normalizer.remove_punctuation1(text)\n",
    "    #print(\"texto\", text)\n",
    "    tokens = normalizer.tokenize_words(text)\n",
    "    #print(\"tokens\", tokens)\n",
    "    tokens = normalizer.remove_stopwords(tokens)\n",
    "    \n",
    "    polarity = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token in positives:\n",
    "            polarity += 1\n",
    "        elif token in negatives:\n",
    "            polarity -= 1\n",
    "    if not binary:\n",
    "        return polarity\n",
    "    else:\n",
    "        if polarity < 0:\n",
    "            polarity = -1\n",
    "        elif polarity > 0:\n",
    "            polarity = 1\n",
    "        \n",
    "        return polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexical_sentment_analysis('Eu estou muito triste e triste')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using dataset of IMDb, available at: http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "import wget\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "\n",
    "filename = \"../dataset/aclImdb_v1.tar.gz\"\n",
    "\n",
    "\n",
    "# Donwload data\n",
    "\n",
    "dataset_link = \"http://ai.stanford.edu/~amaas/data/sentiment/{}\".format(\"aclImdb_v1.tar.gz\")\n",
    "try:\n",
    "    os.mkdir(\"../dataset\")\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "if not os.path.isfile(filename):\n",
    "    file = wget.download(dataset_link, out='../dataset/aclImdb_v1.tar.gz')\n",
    "    tar = tarfile.open(filename, \"r:gz\")\n",
    "    tar.extractall(\"../dataset\")\n",
    "    tar.close()\n",
    "\n",
    "\n",
    "# read data\n",
    "\n",
    "dataset_path = '../dataset/aclImdb'\n",
    "train_positive_files = ['train/pos/'+f for f in os.listdir(dataset_path+'/train/pos') \\\n",
    "                        if os.path.isfile(os.path.join(dataset_path+'/train/pos', f))]\n",
    "\n",
    "train_negative_files = ['train/neg/'+f for f in os.listdir(dataset_path+'/train/neg') \\\n",
    "                        if os.path.isfile(os.path.join(dataset_path+'/train/neg', f))]\n",
    "\n",
    "test_positive_files = ['test/pos/'+f for f in os.listdir(dataset_path+'/test/pos') \\\n",
    "                       if os.path.isfile(os.path.join(dataset_path+'/test/pos', f))]\n",
    "\n",
    "test_negative_files = ['test/neg/'+f for f in os.listdir(dataset_path+'/test/neg') \\\n",
    "                       if os.path.isfile(os.path.join(dataset_path+'/test/neg', f))]\n",
    "\n",
    "all_files = list(set().union(train_positive_files,train_negative_files, test_positive_files, test_negative_files))\n",
    "\n",
    "dataset = {'trainset':[], 'polarity':[], 'bin_polarity': [], 'review':[]}\n",
    "\n",
    "for file in all_files:\n",
    "    polarity = file.split('.')[0].split('_')[1]\n",
    "    with open(os.path.join(dataset_path, file), 'r') as text_file:\n",
    "        dataset['trainset'].append(file.split('/')[0])\n",
    "        bin_polarity = 1 if int(polarity) > 5 else 0  # transform into binary polarity\n",
    "        dataset['bin_polarity'].append(bin_polarity)\n",
    "        dataset['polarity'].append(polarity)\n",
    "        dataset['review'].append(text_file.readlines()[0])\n",
    "\n",
    "        \n",
    "# create dataframe\n",
    "\n",
    "dataframe = pd.DataFrame(data=dataset)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = stopwords.words('english')\n",
    "\n",
    "def preprocessing(text):\n",
    "    text = normalizer.lowercase(text)\n",
    "    text = normalizer.remove_punctuation(text)\n",
    "    tokens = normalizer.tokenize_words(text)\n",
    "    tokens = [token for token in tokens if token not in english_stopwords]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "dataframe['normalized_review'] = dataframe['review'].apply(preprocessing)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_reviews = dataframe[dataframe['trainset'] == 'train']['normalized_review'].values.tolist()\n",
    "train_classes = dataframe[dataframe['trainset'] == 'train']['bin_polarity'].values.tolist()\n",
    "test_reviews = dataframe[dataframe['trainset'] == 'test']['normalized_review'].values.tolist()\n",
    "test_classes = dataframe[dataframe['trainset'] == 'test']['bin_polarity'].values.tolist()\n",
    "\n",
    "transformer = TfidfVectorizer()\n",
    "transformer.fit(train_reviews)\n",
    "X = transformer.transform(train_reviews)\n",
    "X_test = transformer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tentar abordagem de aprendizado utilizando os reviews positivos e negativos passando para o algoritmo\n",
    "#aprender com base no w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifier = SVC(kernel='linear')\n",
    "classifier = SVC()\n",
    "classifier.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_lr = LogisticRegression()\n",
    "#n_jobs=-1\n",
    "classifier_lr.fit(X, train_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(test_classes, classifier_lr.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This film was really bad!\"\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "print(preprocessed_sentence)\n",
    "instance = transformer.transform([preprocessing(sentence)])\n",
    "#predicao de valor 0 tem polaridade negativa\n",
    "print(instance)\n",
    "classifier.predict(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Good film!\"\n",
    "preprocessed_sentence = preprocessing(sentence)\n",
    "print(preprocessed_sentence)\n",
    "instance = transformer.transform([preprocessing(sentence)])\n",
    "print(instance)\n",
    "classifier.predict(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

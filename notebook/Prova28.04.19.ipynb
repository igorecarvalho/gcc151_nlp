{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prova de PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nlputils import lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definição do diretorio dos corpus e criacao de uma lista com os nomes de cada arquivo dentro do diretorio\n",
    "corpora_path = '../data/corpora/'\n",
    "files_corpora = os.listdir(corpora_path)\n",
    "files_corpora = [d for d in files_corpora if d not in '.DS_Store']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação\tde\tuma\tbiblioteca\tde\tPLN,\tcom\trotinas\tde\tnormalização\ttextual\tdo\tnível\tlexical:\n",
    "- remoção\tde\tpontuação\n",
    "- remoção\tde\tacentos\n",
    "- remoção\tde\tstopwords\n",
    "- lowercase\n",
    "- stemming\n",
    "- tokenizar\ttexto\tem\tsentenças\n",
    "- tokenizer\ttexto\tem\tpalavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import nltk\n",
    "nltk.download('rslp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chamada da bibioteca de preprocessamento\n",
    "normalizer = lexical.Preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cada\tcórpus\tdeve\testar\tem\tum\tdiretório\tespecífico e\tconter,\tpelo\tmenos\t500 textos\tcada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criacao de um dicionario que ira armazenar cada corpus em uma chave\n",
    "sentences_dic = {}\n",
    "all_files = []\n",
    "for corpus in files_corpora:\n",
    "    files = [os.path.join(corpora_path + corpus, f) \\\n",
    "             for f in os.listdir(corpora_path + corpus) \\\n",
    "             if os.path.isfile(os.path.join(corpora_path + corpus, f))]\n",
    "    #cada corpus tera mais 3 chaves para armazenar informacoes de trabalho\n",
    "    sentences_dic[corpus] = {'sentencas': [], 'tokens': [], 'tamanho': []}\n",
    "    \n",
    "    #adiciona todos os arquivos em uma unica lista independentemente do corpus\n",
    "    print(corpus)\n",
    "    print(len(files))\n",
    "    all_files.extend(files)\n",
    "    \n",
    "    #para cada arquivo em um corpus sera extraido suas frases e armazenadas em cada linha de uma lista\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as text_file:\n",
    "            lines = text_file.readlines()\n",
    "            for line in lines:\n",
    "                if line != '\\n':\n",
    "                    #armazenamento das sentencas do arquivo como escritas originalmente\n",
    "                    sentences_dic[corpus]['sentencas'].append(line)\n",
    "\n",
    "                    #toda a sentenca sera escrita em letras minusculas\n",
    "                    line = normalizer.lowercase(line) \n",
    "                    #tokeniza as sentencas\n",
    "                    sentences = normalizer.tokenize_sentences(line)\n",
    "                    \n",
    "                    #remove as pontuacoes\n",
    "                    sentences = normalizer.remove_punctuation(sentences)\n",
    "                    \n",
    "                    #tokeniza as palavras de cada sentenca\n",
    "                    sentences = [normalizer.tokenize_words(sent) for sent in sentences]\n",
    "                    #print(\"numero sentencas:\", len(sentences))\n",
    "                    #remove os stopwords\n",
    "                    sentences = normalizer.remove_stopwords(sentences)\n",
    "                    \n",
    "                    #armazena cada sentenca em forma de tokens\n",
    "                    print(sentences)\n",
    "                    sentences_dic[corpus]['tokens'].append(sentences[0])\n",
    "                    #armazena o tamanho, em numero de palavras, de cada sentenca\n",
    "                    sentences_dic[corpus]['tamanho'].append(len(sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilizacao do Pandas para visualizao dos dados em forma de tabelas\n",
    "import pandas as pd\n",
    "\n",
    "#cracao de um dicionario que ira armazenar cada corpus em suas respectivas keys.\n",
    "dataframes_dic = {}\n",
    "for key in sentences_dic.keys():\n",
    "    #os corpus armazenados aqui estara em formato de DataFrame onde cada key sera uma coluna da tabela\n",
    "    dataframes_dic[key] = pd.DataFrame(sentences_dic[key], columns=['sentencas','tokens','tamanho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dic['animais'].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes_dic['games'].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criacao de uma \"bag\" de palavras dos tokens para cada corpus\n",
    "dic_words = {}\n",
    "for key in sentences_dic.keys():\n",
    "    words_corpus = []\n",
    "    for sentence in sentences_dic[key]['tokens']:\n",
    "        no_stopwords_sentence = normalizer.remove_stopwords(sentence)\n",
    "        words_corpus.extend(no_stopwords_sentence)\n",
    "    #adiciona ao dicionario de palavras o corpus de tokens\n",
    "    dic_words[key] = words_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilização do sklearn para facilitar a contagem de palavras com auxilio da bibioteca CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#função que realiza a contagem das palavras e retona 3 lista:\n",
    "# - Lista de frenquencia de palavras(words_freq)\n",
    "# - Lista das 20 palavras mais frequentes(most_freq_words)\n",
    "# - Lista das 20 palavras menos frequentes(less_freq_words)\n",
    "def words_frequency(token_corpus):\n",
    "    vec = CountVectorizer().fit(token_corpus)\n",
    "\n",
    "    #Here we get a Bag of Word model that has cleaned the text, removing non-aphanumeric characters and stop words.\n",
    "    bag_of_words = vec.transform(token_corpus)\n",
    "\n",
    "    #sum_words is a vector that contains the sum of each word occurrence in all texts in the corpus. \n",
    "    #In other words, we are adding the elements for each column of bag_of_words matrix.\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "\n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    most_freq_words = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    less_freq_words = sorted(words_freq, key = lambda x: x[1])\n",
    "    \n",
    "    return words_freq, most_freq_words[:20], less_freq_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicionario que ira armazenar as inforções das palavras em cada corpus\n",
    "word_analysis = {}\n",
    "\n",
    "#dicionario de dataFrame das palavras listadas no dicionario word_analysis\n",
    "word_df = {}\n",
    "\n",
    "for key in dic_words.keys():\n",
    "    #as listas retornadas pela função words_frequency é uma lista de lista que indica a palavra juntamente com sua\n",
    "    #contagem de aparição no texto\n",
    "    words_freq, most_freq_words, less_freq_words = words_frequency(dic_words[key])\n",
    "    \n",
    "    word_analysis[key] = {'Palavra': [],'Quantidade': [], 'Tamanho': []}\n",
    "    \n",
    "    #por palavra retornada da lista armazeno no dicionario separadamente a palavra, sua quantidade e seu tamanho\n",
    "    for wd_qt in words_freq:\n",
    "        word_analysis[key]['Palavra'].append(wd_qt[0])\n",
    "        word_analysis[key]['Quantidade'].append(wd_qt[1])\n",
    "        word_analysis[key]['Tamanho'].append(len(wd_qt[0]))\n",
    "\n",
    "    word_df[key] = pd.DataFrame(word_analysis[key], columns=['Palavra','Quantidade','Tamanho'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df['animais'].head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df['games'].head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As\tseguintes\testatísticas\tdevem ser\tapresentadas, por\tcórpus:\n",
    "- 20\tpalavras\tmais\tfrequentes\n",
    "- 20\tpalavras\tmenos\tfrequentes\n",
    "- Tamanho\tmédio\tdas\tpalavras\n",
    "- Tamanho\tmédio\tdas\tsentenças,\tem\tnúmero\tde\tpalavras\n",
    "- Outras\tduas\testatísticas\tque\tachar\tinteressante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dicionario para armazenar as estatisticas sobre os corpus\n",
    "corpus_analysis = {}\n",
    "\n",
    "#dicionario que servirá para armazenar dataframes de estatisticas de cada corpus\n",
    "analysis_df = {}\n",
    "for corpus in sentences_dic.keys():\n",
    "    #as listas retornadas pela função words_frequency é uma lista de lista que indica a palavra juntamente com sua\n",
    "    #contagem de aparição no texto\n",
    "    words_freq, mfw, lfw = words_frequency(dic_words[corpus])\n",
    "    \n",
    "    #dicionario de dicionario, onde cada corpus terá suas proprioas estatisticas armazenadas\n",
    "    corpus_analysis[corpus] = {'Palavras Mais Frequentes(20)': [], 'Palavras Menos Frequentes(20)': [], \n",
    "                               'Tamanho medio das Palavras': [], 'Numero de sentencas': [], \n",
    "                               'Tamanho medio das sentencas': [], 'Word2Vec': []}\n",
    "    \n",
    "    #separa da lista de lista somente as palavra, armazena um vetor e salva no dicionario\n",
    "    most_freq_words = []\n",
    "    for w in mfw:\n",
    "        most_freq_words.append(w[0])\n",
    "    corpus_analysis[corpus]['Palavras Mais Frequentes(20)'].append(most_freq_words)\n",
    "    \n",
    "    #separa da lista de lista somente as palavra, armazena um vetor e salva no dicionario\n",
    "    less_freq_words = []\n",
    "    for w in lfw:\n",
    "        less_freq_words.append(w[0])\n",
    "    corpus_analysis[corpus]['Palavras Menos Frequentes(20)'].append(less_freq_words)\n",
    "    \n",
    "    #com uso do dataframe word_df é calculada a media do tamanho de todas as palavras do corpus adicionado ao\n",
    "    #dicionario de estatistica\n",
    "    words_mean = word_df[corpus]['Tamanho'].mean()\n",
    "    corpus_analysis[corpus]['Tamanho medio das Palavras'].append(words_mean)\n",
    "    \n",
    "    #adiciona o numero de sentencas ao dicionario de estatisticas\n",
    "    corpus_analysis[corpus]['Numero de sentencas'].append(len(sentences_dic[corpus]['sentencas']))\n",
    "    \n",
    "    #com uso do dataframe word_df é calculada a media do tamanho de todas as sentencas do corpus adicionado ao\n",
    "    #dicionario de estatistica\n",
    "    sentences_mean = dataframes_dic[corpus]['tamanho'].mean()\n",
    "    corpus_analysis[corpus]['Tamanho medio das sentencas'].append(sentences_mean)\n",
    "    \n",
    "    #criacao do dataframe com os dados estatisticos adicinados ao dicinario corpus_analysis\n",
    "    analysis_df[corpus] = pd.DataFrame(corpus_analysis[corpus], columns=['Palavras Mais Frequentes(20)',\n",
    "                                                                        'Palavras Menos Frequentes(20)',\n",
    "                                                                       'Tamanho medio das Palavras',\n",
    "                                                                       'Numero de sentencas',\n",
    "                                                                       'Tamanho medio das sentencas'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exibe as estatisticas para todos os corpus\n",
    "for corpus in corpus_analysis.keys():\n",
    "    print(\"\\nEstátisticas do corpus\", corpus)\n",
    "    print(\"20 palavras mais frequentes: \\n\", corpus_analysis[corpus]['Palavras Mais Frequentes(20)'])\n",
    "    print(\"20 palavras menos frequentes: \\n\", corpus_analysis[corpus]['Palavras Menos Frequentes(20)'])\n",
    "    print(\"Tamanho medio das Palavras: \", corpus_analysis[corpus]['Tamanho medio das Palavras'])\n",
    "    print(\"Numero de sentencas: \", corpus_analysis[corpus]['Numero de sentencas'])\n",
    "    print(\"Tamanho medio das sentencas: \", corpus_analysis[corpus]['Tamanho medio das sentencas'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exibe o dataframe de estatisticas para os corpus\n",
    "analysis_df['animais'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df['games'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalização\tdos\tcorpora,\tcom\tvistas\tà\tcriação\tde\tdois\tmodelos,\ta\tsaber,\tWord2Vec\te\tDoc2Vec.\n",
    "- Para\tcada\tcórpus, será criado\tum\tmodelo\tWord2Vec\n",
    "- Para\ttodos\tos\tcorpora,\tapenas\tum\tmodelo\tDoc2Vec\tserá criado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para\tcada\tcórpus, será criado\tum\tmodelo\tWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importacao das bibiotecas w2vec e d2vec\n",
    "from gensim.models import Word2Vec, Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para cada corpus é adicionado em seu dicionario um modelo de word2vec \n",
    "for corpus in corpus_analysis.keys():\n",
    "    sentence_tokens = sentences_dic[corpus]['tokens']\n",
    "    w2vmodel = Word2Vec(sentences=sentence_tokens, size=300,min_count=5, workers=4, window=2)\n",
    "    corpus_analysis[corpus]['Word2Vec'] = w2vmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para todos os corpora, apenas um modelo Doc2Vec será criado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- preparação dos documentos em uma lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lista que recebera cada documentos de todos os corpora em cada uma de suas posicoes\n",
    "all_documents = []\n",
    "for file in all_files:\n",
    "    with open(file, 'r', encoding='utf-8') as text_file:\n",
    "        document = ' '.join(text_file.readlines())\n",
    "        document = normalizer.lowercase(document)\n",
    "        document_tokens = normalizer.tokenize_words(document)\n",
    "        all_documents.append(document_tokens)\n",
    "print(\"Number of documents: {}\".format(len(all_documents)))\n",
    "\n",
    "#enumera os documentos taggeando-os e salvando-os em tagged_documents\n",
    "tagged_documents = [TaggedDocument(words=d, tags=[str(i)]) for i, d in enumerate(all_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- treino do modelo de doc2vec para todos os arquivos do corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treina um modelo de doc2vec apartir da lista de documentos tageados\n",
    "d2vmodel = Doc2Vec(tagged_documents, vector_size=20, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso\tdos\tmodelos\tgerados\n",
    "\n",
    "- Uso\tdo\tWord2Vec\n",
    "\n",
    "#### Dada\tuma\tpalavra w1 de\tum\tcórpus,\tquais\tas\t10\tpalavras\tmais\tsimilares a\tw1?\n",
    "1. Exemplifique\t com\t três\t palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a quantidade de palavras similares deseja buscar\n",
    "n_words = 10\n",
    "\n",
    "#palavras para encontrar similaridade de acordo com os modelos treinados com o word2vec\n",
    "test_word = [['água', 'alimentação', 'cuidados'],['jogos', 'jogadores', 'online']]\n",
    "\n",
    "cont = 0\n",
    "for corpus in corpus_analysis:\n",
    "    print(\"\\nCorpus\", corpus)\n",
    "    print(\"\\nAs \", n_words, \" palavras mais similares a \", test_word[cont][0], \"sao:\")\n",
    "    print(corpus_analysis[corpus]['Word2Vec'].wv.most_similar(test_word[cont][0], topn=n_words))\n",
    "    print(\"\\nAs \", n_words, \" palavras mais similares a \", test_word[cont][1], \"sao:\")\n",
    "    print(corpus_analysis[corpus]['Word2Vec'].wv.most_similar(test_word[cont][1], topn=n_words))\n",
    "    print(\"\\nAs \", n_words, \" palavras mais similares a \", test_word[cont][2], \"sao:\")\n",
    "    print(corpus_analysis[corpus]['Word2Vec'].wv.most_similar(test_word[cont][2], topn=n_words))\n",
    "    cont = cont+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. discuta\t como\t poderia\tmelhorar\t os\tresultados.\tPense\tno\tnível\tda\tmorfologia\tou\toutro\tdo\tPLN.\n",
    "\n",
    "Para melhorar o resultado do word2vec é utilizar um corpus maior, com mais palavras, outros meios tambem é melhorar a normatização dos dados removendo stopwords, colocar as palavras em minusculas e utiliar uma boa fonte para extração dos corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Tentar\talguma\tabordagem\tpara\tcomparar\tdois\tdocumentos\tdiferentes\tutilizando\tos\tvetores\tdo\tWord2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Uso\tdo\tDoc2Vec\n",
    "\n",
    "#### Dados\tos\tdocumentos\t(textos) de\tcorporas\tdiferentes,\tutilize\tos\tvetores\tpara encontrar\tos\tdocumentos\tmais\tsimilares\n",
    "\n",
    "1. Exemplifique\tcom\ttrês\tdocumentos\te\tdiscuta\tos resultados.\tAo\tser\tver, foram\tbons?\tOs\tdocumentos\trealmente\tsão\tparecidos?\tO\tque\tpoderia\t ser\tfeito\tpara\tmelhorar os\tresultados?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numero de documentos similares que deseja procurar\n",
    "n_similar = 5\n",
    "\n",
    "#posicao do documento na lista all_documentos que deseja procurar documentos similares\n",
    "n_doc = 50\n",
    "n_doc1 = 950\n",
    "n_doc2 = 498\n",
    "\n",
    "print(\"\\ndocumentos similares ao documento: \", all_files[n_doc])\n",
    "similar_docs = d2vmodel.docvecs.most_similar(n_doc, topn=n_similar)\n",
    "for doc_num in similar_docs:\n",
    "    num_doc = int(doc_num[0])\n",
    "    print(all_files[num_doc])\n",
    "    \n",
    "print(\"\\ndocumentos similares ao documento: \", all_files[n_doc1])\n",
    "similar_docs = d2vmodel.docvecs.most_similar(n_doc1, topn=n_similar)\n",
    "for doc_num in similar_docs:\n",
    "    num_doc = int(doc_num[0])\n",
    "    print(all_files[num_doc])\n",
    "\n",
    "print(\"\\ndocumentos similares ao documento: \", all_files[n_doc2])\n",
    "similar_docs = d2vmodel.docvecs.most_similar(n_doc2, topn=n_similar)\n",
    "for doc_num in similar_docs:\n",
    "    num_doc = int(doc_num[0])\n",
    "    print(all_files[num_doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
